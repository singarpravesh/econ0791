---
title: "Linear Regression"
format: 
  revealjs:
    warning: false
    message: false
    echo: true
    scrollable: true
    slide-number: true
    touch: true
editor_options: 
  chunk_output_type: console
---

## Problem Statement

The superintendent of an elementary school district must decide whether to hire additional teachers and she wants your advice. If she hires the teachers, she will reduce the number of students per teacher (the student–teacher ratio) by two. She faces a trade-off. Parents want smaller classes so that their children can receive more individualized attention. But hiring more teachers means spending more money, which is not to the liking of those paying the bill! So she asks you: If she cuts class sizes, what will the effect be on student performance? 

## {.smaller}

- For a sample of $n$ districts, we can write
$$
\text{TestScore} = \beta_0 + \beta_1\text{ClassSize}+ \\ \text{Other factors that infulence student-teacher ratio} 
$$

$$
y = \beta_0 + \beta_1x+ u
$$

---

```{r}
library(AER) 
data('CASchools', package = "AER")
CASchools
```

---

```{r}
library(tidyverse)
as_tibble(CASchools) -> caschools
caschools
```

## Estimating the coefficients 

- Data description
  - Consists of class sizes and test scores in 1998-99 in 420 California districts.
  - **Classes:** 6th and 8th grade students
  - **Class size:** $\frac{\text{Number of students in the district}}{\text{Number of teachers in the district}}$ i.e. district wise student-teacher ratio.

## {.smaller}

- **Task:** Summarize the Distribution of Student–teacher Ratios and 5th-Grade test Scores for 420 Districts in California in 1998-99. *This is Table 4.1 in the book. Page no 161.*
```{r}
#| echo: false
# create new variables "str" and "testscr".
caschools %>% 
  mutate(str = students/teachers,
         testscr = (english+math)/2) -> caschools

# summary table 
caschools %>% 
  pivot_longer(cols = c(str, testscr), names_to = "variables", values_to = "value") %>% 
  group_by(variables) %>% 
  summarise(Average = mean(value),
            `Standard Deviation` = sd(value),
            `Percentile 10%` = quantile(value, 0.1),
            `Percentile 25%` = quantile(value, 0.25),
            Median = quantile(value, 0.5),
            `Percentile 75%` = quantile(value, 0.75)) 
```
- Observations:
  - 10% of districts have student-teacher ratio below 17.3.
  - 50% of the districts have student-teacher ratio below 19.7.

---

- Steps:
  1. Create new variables `str` and `testscr`.
  3. Tidy the data into long format using the variables `str` and `testscr`, and then group them and summarize. You can use `sd()` for standard deviation, `quantile()` for percentiles, and `mean()` for average.
  
---

1. Create new variables `str` and `testscr`.
```{r}
#| code-fold: true
# Create new variables "str" and "testscr".
caschools %>% 
  mutate(str = students/teachers,
         testscr = (read+math)/2) -> caschools
caschools
```

---

2. Tidy and Summarize
```{r}
#| code-fold: true
# summarize
caschools %>% 
  pivot_longer(cols = c(str, testscr), names_to = "variables", values_to = "value") %>% 
  group_by(variables) %>% 
  summarise(Average = mean(value),
            `Standard Deviation` = sd(value),
            `Percentile 10%` = quantile(value, 0.1),
            `Percentile 25%` = quantile(value, 0.25),
            Median = quantile(value, 0.5),
            `Percentile 75%` = quantile(value, 0.75))
```

  
## {.smaller}

- How are the two variables `testscr` and `str` related? Hint:Use scatter plot and correlation coefficient.

```{r}
#| code-fold: true
 
caschools %>% 
  ggplot(aes(x = str, y = testscr))+
  geom_point()+
  labs(x = "Student-Teacher Ratio",
       y = "Test Scores",
       title = 'Scatterplot of test Score vs. Student–teacher Ratio (California School District Data)')
```

- The correlation coefficient is `r round(cor(caschools$str, caschools$testscr),2)`
```{r}
#| code-fold: true
#| eval: false
cor(caschools$str, caschools$testscr)
```


## OLS Estimates {.smaller}

- The OLS equation to be estimated
$$
testscr = \beta_0 + \beta_1 str+ u 
$$


```{r}
lm(testscr ~ str, data = caschools) 
```

- Interpretation:
  - The slope of -2.28 means that an increase in the student–teacher ratio by one student per class is, on average, associated with a decline in districtwide test scores by 2.28 points on the test. 
  - The negative slope indicates that more students per teacher (larger classes) is associated with poorer performance on the test.

## The estimated Regression Line for the California Data {.smaller}

```{r}
#| code-fold: true
caschools %>% 
  ggplot(aes(str, testscr))+
  geom_point()+
  geom_smooth(method = "lm",
              formula = y ~ x, se = FALSE)
```

## Measures of fit {.smaller}

- The $R^2$ and the standard error of the regression measure how well the regression line fits the data.
- The $R^2$ ranges between 0 and 1 and measures the fraction of the variance of $Y_i$ that is explained by $X_i$.
  - An $R^2$ near 1 indicates that the regressor is good at predicting $Y_i$, while an $R^2$ near 0 indicates that the regressor is not  very good at predicting $Y_i$. 
- The standard error of the regression measures how far $Y_i$ typically is from the predicted value.


## {.smaller}

- The regression summary of the linear regression
```{r}
#| code-fold: true
lm(testscr ~ str, data = caschools) %>% 
  summary()
```

- Interpretation
  - The $R^2$ of 0.051 means that the regressor `str` explains $5.1$% of the variance of the dependent variable `testscre`.
  -  The SER of 18.6 means that there is a large spread of the scatterplot around the regression line as measured in points on
the test. 
  - The low $R^2$ and high SER indicate that  the student–teacher ratio alone explains only a small part of the variation in test scores in these data.


## The Least Squares assumptions

$$
Y_i = \beta_0 + \beta_1X_i+ u_i, i = 1, 2, ..., n
$$

1. The error term $u_i$ has conditional mean zero given $X_i$: $E(u_i|X_i) = 0$
2. $(X_i, Y_i)$ are independently and identically distributed (i.i.d)
3. Large outliers are unlikely.


## Exercises {.smaller}

The file [Growth](LinearRegression_files/Growth.xlsx), contains data on average growth rates from 1960 through 1995 for 65 countries, along with variables that are potentially related to growth. A detailed description is given in [Growth_Description](LinearRegression_files/Growth_Description.pdf). In this exercise, you will investigate the relationship   between growth and trade.

a. Construct a scatterplot of average annual growth rate (Growth) on the average trade share (TradeShare). Does there appear to be a relationship between the variables? 

b. One country, Malta, has a trade share much larger than the other countries. Find Malta on the scatterplot. Does Malta look like an outlier? 

c. Using all observations, run a regression of Growth on TradeShare. What is the estimated slope? What is the estimated intercept? Use the regression to predict the growth rate for a country with a trade share of 0.5 and with a trade share equal to 1.0. 

d. Estimate the same regression, excluding the data from Malta. Answer the same questions in (c). 

e. Plot the estimated regression functions from (c) and (d). Using the scatterplot in (a), explain why the regression function that includes Malta is steeper than the regression function that excludes Malta. 

f. Where is Malta? Why is the Malta trade share so large? Should Malta be included or excluded from the analysis? 

# Regression with a Single Regressor: Hypothesis Tests and Confidence Intervals

## Testing Hypotheses About One of the Regression Coefficients

- An angry taxpayer asserts that cutting class size will not help boost test scores, so reducing them is a waste of money. 
- Class size, the taxpayer claims, has no effect on test scores.
- Can you reject the taxpayer’s hypothesis that $\beta_{ClassSize} = 0$, or should you accept it, at least tentatively pending further new evidence?


## Two-Sided Hypotheses Concerning $\beta_1$ {.smaller}

- The null hypothesis that the mean of $Y$ is a specific value $\mu_{Y,0}$ can be written as $H_0: E(Y) = \mu_{Y,0}$, and the two-sided alternative is $H_1: E(Y) \neq \mu_{Y,0}$.
- Testing the Hypothesis $\beta_1 = \beta_{1,0}$ against the alternative $\beta_1 \neq \beta_{1,0}$

1. Compute the standard error of $\hat\beta_1$, $SE(\hat\beta_1)$.
$$SE(\hat{\beta}_1) = \sqrt{ \hat{\sigma}^2_{\hat{\beta}_1} } \ \ , \ \ 
  \hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \overline{X})^2 \hat{u_i}^2 }{ \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \right]^2}$$.
2. Compute the $t$-statistic.
$$
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{ SE(\hat{\beta}_1) }.
$$
3. Compute the $p$-value. Reject the hypothesis at the 5% significance level if the $p$-value is less than 0.05 or, equivalently, if $|t^{act}|>1.96$.
$$
\begin{align*}
    p \text{-value} =& \, \text{Pr}_{H_0} \left[ \left| \frac{ \hat{\beta}_1 - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| > \left|        \frac{ \hat{\beta}_1^{act} - \beta_{1,0} }{ SE(\hat{\beta}_1) } \right| \right] \\
    =& \, \text{Pr}_{H_0} (|t| > |t^{act}|) \\
    \approx& \, 2 \cdot \Phi(-|t^{act}|)
  \end{align*}
$$






## Application to the test scores data {.smaller}

- Recall our regression estimates
```{r}
#| eval: false
lm(testscr ~ str, data = caschools)
```

- Now, we shall look at the standard way of printing the results
```{r}
library(stargazer)
lm(testscr ~ str, data = caschools) %>% 
  stargazer(type = "text")
```

## {.smaller}

- Suppose you wish to test the null hypothesis that the slope $\beta_1 = 0$ **(two-sided test)** at the 5% significance level.  $H_0: \beta_1 =0, H_1: \beta_1 \neq 0 $
```{r}
# the linear model
(model1 <- lm(testscr ~ str, data = caschools))

# the coefficient on str
(beta1 <- coef(model1)[[2]])

# the standard error of beta 1
(se_beta1 <- vcov(model1) %>% diag() %>% sqrt())

# degrees of freedom
(df <- model1$df.residual)

# the t statistic
(t <- (beta1 - 0)/se_beta1)

# critical value of t
alpha <- 0.05
(t_cri <- qt(1 - alpha/2, df))
```

- Interpretation
  - Since *|t| > t_cri*, the *t* value falls in the rejection region, so we reject the null hypothesis. 
  
---

```{r}
#| code-fold: true
ggplot(tibble(-4:4), aes(x=-4:4))+
  geom_function(aes(x=-4:4),fun = dnorm)+
  geom_vline(xintercept = t[[2]], lty = 2)+
  geom_vline(xintercept = t_cri, col = 'red', lty = 2)+
  geom_vline(xintercept = -t_cri, col = 'red', lty = 2)+
  annotate("text", x = t[[2]]+0.3, y = 0.4, label = "t = -4.75")+
  annotate("text", x = t_cri+0.5, y = 0.4, label = "t_cri = 1.96")+
  annotate("text", x = - t_cri+0.5, y = 0.4, label = "t_cri = -1.96")+
  labs(x = "t",
       y = "density",
       title = "Two-tail test")
```


---

- Calculating the $p$-value for two-tail test
```{r}
(p <- 2*(1 - pt(abs(t), df)))[[2]]
```

- Since the $p$-value is less than 0.05 we reject the null hypotheses that $\beta_1  = 0$.


## {.smaller}

- Now you wish to test the null hypothesis that the slope $\beta_1<3$ **(one-sided test)** at the 5% significance level. $H_0: \beta_1 \geq3, H_1:\beta_1<3$
- All the values of $\beta_1, SE(\beta_1), \text{ and }df$ remain the same. We just need to calculate the one-sided *t* and the critical value and interpret.
```{r}

# critical value of t 
alpha <- 0.05
(t_one <- (beta1 - 3)/se_beta1)
(t_cri_one <- qt(1 - alpha, df))
```

---

```{r}
#| code-fold: true
ggplot(tibble(-4:4), aes(x=-4:4))+
  geom_function(aes(x=-4:4),fun = dnorm)+
  geom_vline(xintercept = t_one[[2]], lty = 2)+
  geom_vline(xintercept = -t_cri_one, col = 'red', lty = 2)+
  annotate("text", x = t_one[[2]]+0.7, y = 0.4, label = paste0("t = ", round(t_one[[2]], 3) ))+
  annotate("text", x = -t_cri_one+0.6, y = 0.4, label = paste0("t = ", round(-t_cri_one, 3) ))+
  labs(x = "t",
       y = "density",
       title = "Left-tail test")
```


---

- Calculating the $p$-value for one-tail (left) test. $H_0: \beta_1 \geq3, H_1:\beta_1<3$
```{r}
(p_left <- pt(t_one, df))[[2]]
```

  - Since the $p$-value is less than 0.05 we reject the null hypotheses that $\beta_1  \geq 3$.

- Calculating the $p$-value for one-tail (right) test. $H_0: \beta_1 \leq3, H_1:\beta_1>3$
```{r}
(p_left <- 1 - pt(t_one, df))[[2]]
```

  - Since the $p$-value is greater than 0.05 we accept the null hypotheses that $\beta_1  \leq 3$.

## Confidence interval  for $\beta_1$

95% confidence interval for $\beta_1 = [\hat\beta_1-1.96SE(\hat\beta_1),\hat\beta_1+1.96SE(\hat\beta_1)]$ 

- For our `model1` we can construct the 95% CI as follows
```{r}
lower_ci_model1 <- beta1 - 1.96*se_beta1[[2]]
upper_ci_model1 <- beta1 + 1.96*se_beta1[[2]]
(ci_model1 <- tibble(lower_ci_model1, upper_ci_model1))
```

## Regression when *X* is a binary variable {.smaller}

- Regression analysis can also be used when the regressor is binary—that is, when it takes on only two values, 0 or 1. 
- For example, X might be a worker’s gender (=1 if female, = 0 if male), whether a school district is urban or rural (= 1 if urban, = 0 if rural), or whether the district’s class size is small or large (= 1 if small, = 0 if large). 
- A binary variable is also called an indicator variable or sometimes a dummy variable. 
- In R we call them factors.


## {.smaller}

- Suppose you have a variable $D_i$ that equals either 0 or 1, depending on whether the student–teacher ratio is less than 20:
$$
D_i = \begin{cases} 1 \text{ if $STR$ in $i^{th}$ school district < 20} \\
0 \text{ if $STR$ in $i^{th}$ school district $\geq$ 20} \end{cases}
$$

- The regression model now is
$$
testscr = \beta_0 + \beta_1D_i+u_i
$$

>- Your task: Create the dummy variable and make an appropriate plot to look at both these variables $D_i$ and $testscr$.

---

```{r}
#| code-fold: true
# create a new factor variable D
caschools %>% 
  mutate(D = factor(
              if_else(str < 20,1,0))) -> caschools

# plot D and testscr
caschools %>% 
  ggplot()+
  geom_boxplot(aes(x = D, y = testscr))+
  labs(title = "Box plot")
```

## {.smaller}

- Now, let's estimate the regression equation
$$
testscr = \beta_0 + \beta_1D_i+u_i
$$

```{r}
lm(testscr ~ D, data = caschools) %>% 
  stargazer::stargazer(type = "text")
```

- Interpretation
  - The average test score for the subsample with student–teacher ratios greater than or equal to 20 (that is, for which D = 0) is 650.0, and the average test score for the subsample with student– teacher ratios less than 20 (so D = 1) is 650.0 + 7.4 = 657.4. 
  - The difference between the sample average test scores for the two groups is 7.4. This is the OLS estimate of $\beta_1$, the coefficient on the student–teacher ratio binary variable D. 

